{"timestamp":"2025-11-10T20:31:47.399958Z","level":"info","event":"DAG bundles loaded: dags-folder, example_dags","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager","filename":"manager.py","lineno":179}
{"timestamp":"2025-11-10T20:31:47.401741Z","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/dag_transiction_to_dw.py","logger":"airflow.models.dagbag.DagBag","filename":"dagbag.py","lineno":593}
{"timestamp":"2025-11-10T20:31:47.792334Z","level":"info","event":"Task instance is in running state","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.793189Z","level":"info","event":" Previous state of the Task instance: TaskInstanceState.QUEUED","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.793822Z","level":"info","event":"Current task name:run_spark_job","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.794335Z","level":"info","event":"Dag name:csv_to_postgres_datalake","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.814474Z","level":"info","event":"Could not load connection string spark_default, defaulting to yarn","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":305}
{"timestamp":"2025-11-10T20:31:47.815210Z","level":"info","event":"Spark-Submit cmd: spark-submit --master yarn --name arrow-spark --verbose /opt/airflow/spark/jobs/transform_to_data_lake.py","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":474}
{"timestamp":"2025-11-10T20:31:47.892207Z","level":"info","event":"JAVA_HOME is not set","logger":"airflow.task.hooks.airflow.providers.apache.spark.hooks.spark_submit.SparkSubmitHook","filename":"spark_submit.py","lineno":644}
{"timestamp":"2025-11-10T20:31:47.892893Z","level":"error","event":"Task failed with exception","logger":"task","filename":"task_runner.py","lineno":972,"error_detail":[{"exc_type":"AirflowException","exc_value":"Cannot execute: spark-submit --master yarn --name arrow-spark --verbose /opt/airflow/spark/jobs/transform_to_data_lake.py. Error code is: 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":920,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1307,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/sdk/bases/operator.py","lineno":416,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/operators/spark_submit.py","lineno":197,"name":"execute"},{"filename":"/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py","lineno":566,"name":"submit"}],"is_group":false,"exceptions":[]}]}
{"timestamp":"2025-11-10T20:31:47.894365Z","level":"info","event":"Task instance in failure state","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.894972Z","level":"info","event":"Task start","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.895464Z","level":"info","event":"Task:<Task(SparkSubmitOperator): run_spark_job>","logger":"task.stdout"}
{"timestamp":"2025-11-10T20:31:47.895882Z","level":"info","event":"Failure caused by Cannot execute: spark-submit --master yarn --name arrow-spark --verbose /opt/airflow/spark/jobs/transform_to_data_lake.py. Error code is: 1.","logger":"task.stdout"}
